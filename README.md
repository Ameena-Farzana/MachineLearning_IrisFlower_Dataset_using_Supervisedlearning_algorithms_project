# MachineLearning_IrisFlower_Dataset_using_Supervisedlearning_algorithms_project

## Iris Dataset
The Iris dataset is a widely used and well-known dataset in the field of machine learning and statistics. It was introduced by British biologist and statistician Ronald A. Fisher in 1936 as an example for discriminant analysis. The dataset is particularly popular for classification and clustering tasks due to its simplicity and distinct features. It is often used as a beginner's dataset for exploring various machine learning algorithms.

The Iris dataset contains measurements of four features from three different species of iris flowers. Each feature represents a different aspect of the flower's morphology. The goal of the dataset is to classify iris flowers into one of three species based on these features.

#### Here are the features included in the Iris dataset:

1. Sepal Length: The length of the flower's sepal (the outer part that protects the flower bud).
2. Sepal Width: The width of the flower's sepal.
3. Petal Length: The length of the flower's petal (the inner, often colorful part of the flower).
4. Petal Width: The width of the flower's petal.

#### The three species of iris flowers in the dataset are:

1. Iris Setosa
2. Iris Versicolor
3. Iris Virginica

Each species has distinct characteristics that differentiate them based on the provided features. The dataset consists of 150 samples, with 50 samples from each species.

The Iris dataset is commonly used for educational purposes, algorithm testing, and showcasing the effectiveness of various machine learning techniques. It serves as a foundational example for understanding data preprocessing, feature engineering, model training, and evaluation in machine learning workflows.


## Algorithms applied

In the context of the Iris dataset, various machine learning algorithms can be applied for classification tasks to predict the species of iris flowers based on their features. Here are some common algorithms that can be used:

1) Logistic Regression: A linear model used for binary and multiclass classification. It estimates the probability that a given input belongs to a certain class.

2) Decision Tree: A tree-like model that splits the dataset into subsets based on feature values. It's a versatile algorithm that can handle both classification and regression tasks.

3) Random Forest: An ensemble method that combines multiple decision trees to improve predictive accuracy and control overfitting.

4) Support Vector Machine (SVM): A classification algorithm that finds a hyperplane that best separates different classes while maximizing the margin between them.

5) K-Nearest Neighbors (KNN): A non-parametric algorithm that assigns a new data point's class based on the majority class among its k nearest neighbors in the feature space.

6) Naive Bayes: A probabilistic algorithm based on Bayes' theorem that makes predictions using conditional probabilities. It assumes feature independence.

These algorithms offer different approaches to classifying the Iris flowers based on their features. Depending on the dataset's characteristics, the number of samples, and the desired level of interpretability, one algorithm may perform better than others.

## Observations

1) Logistic Regression: The accuracy of the model was approximately 96.46%.

2) Decision Tree: The accuracy of the model was around 96.39%.

3) Random Forest: The accuracy of the predicted model was about 96.23%.

4) Support Vector Machine (SVM): The accuracy of the model was roughly 93.65%.

5) K-Nearest Neighbors (KNN): The accuracy of the model was approximately 98.08%.

6) Naive Bayes: The accuracy of the model was around 96.0%.

These accuracy results give insights into how well each algorithm performed in classifying the Iris flowers into their respective species. K-Nearest Neighbors (KNN) seems to have achieved the highest accuracy in this case, followed closely by Logistic Regression and Naive Bayes. Random Forest and Decision Tree also show competitive accuracy values. The Support Vector Machine (SVM) achieved a slightly lower accuracy compared to the other algorithms.







